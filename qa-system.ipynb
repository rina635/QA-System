{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "AIT 590 - Assignment 4\n",
    "Team 3 - Rafeef Baamer, Ashish Hingle, Rina Lidder, & Andy Nguyen\n",
    "Date: 4/21/2021\n",
    "Description:\n",
    "Types of questions:\n",
    "    \n",
    "Libraries used: \n",
    "Additional features (for extra credit):    \n",
    "Usage Instructions:     \n",
    "Algorithm defined in program:\n",
    "    \n",
    "Resources used for this assignment come from the materials provided in the AIT 590 course materials.\n",
    "- Lecture powerpoints (AIT 590)\n",
    "- Stanford University Prof. Dan Jurafsky's Video Lectures (https://www.youtube.com/watch?v=zQ6gzQ5YZ8o)\n",
    "- Joe James Python: NLTK video series (https://www.youtube.com/watch?v=RYgqWufzbA8)\n",
    "- w3schools Python Reference (https://www.w3schools.com/python/)\n",
    "- regular expressions 101 (https://regex101.com/)\n",
    "\"\"\"\n",
    "import en_core_web_sm\n",
    "import webbrowser\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "from pprint import pprint\n",
    "import bs4 as bs  # BeautifulSoup\n",
    "import urllib.request\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Command line arguments to run file and store user's questions into log file.\n",
    "#run_file = sys.argv[1]\n",
    "#log_output = sys.argv[2]\n",
    "\n",
    "wiki = \"https://en.wikipedia.org/w/index.php?search={}\"\n",
    "\n",
    "#Retrieve webpage text from Wikipedia site the user's answer leads tos\n",
    "def scrape_webpage(url):\n",
    "    url = url.replace(\" \", \"%20\")\n",
    "    scraped_textdata = urllib.request.urlopen(url)\n",
    "    textdata = scraped_textdata.read()\n",
    "    parsed_textdata = bs.BeautifulSoup(textdata,'lxml')\n",
    "    #paragraphs = parsed_textdata.find_all('p')\n",
    "    paragraphs = parsed_textdata.find_all('p')\n",
    "    formated_text = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        formated_text += para.text\n",
    "    \n",
    "    formated_text = re.sub(r'(\\[.*\\])', '', formated_text)\n",
    "    formated_text = formated_text.replace('\\n', '')\n",
    "    return formated_text.encode('ascii', 'ignore')\n",
    "\n",
    "#Retrieve NER \n",
    "def find_ner(input):\n",
    "    \n",
    "    named_text = ''\n",
    "    named_label = ''\n",
    "    \n",
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "    nlp = en_core_web_sm.load()\n",
    "  \n",
    "    # transform the text to spacy doc format\n",
    "    mytext = nlp(input)\n",
    "    \n",
    "    for ent in mytext.ents:\n",
    "        named_text = ent.text\n",
    "        named_label = ent.label_\n",
    "        \n",
    "        #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "    return named_text, named_label\n",
    "\n",
    "#Retrieve NER \n",
    "def find_ner2(input):\n",
    "    named_label = ''\n",
    "    \n",
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "    nlp = en_core_web_sm.load()\n",
    "  \n",
    "    # transform the text to spacy doc format\n",
    "    mytext = nlp(input)\n",
    "    \n",
    "    for ent in mytext.ents:\n",
    "        named_label = ent.label_\n",
    "        \n",
    "    return named_label\n",
    "\n",
    "# checks the type of question\n",
    "question_words = [\"who\", \"when\", \"where\", \"what\"]\n",
    "\n",
    "#Check the question type and length. If the question in less than three words or starts with other than (who, when, where, what) words\n",
    "#the question will be considered invalid \n",
    "def check_q_type(question):\n",
    "    question = question.lower()\n",
    "    question_token = word_tokenize(question)\n",
    "    question_word = question_token[0]\n",
    "    if question_word not in question_words and len(question_token) < 3:\n",
    "        print(\"This is invalid question\")\n",
    "    else:\n",
    "        if question_word == \"who\":\n",
    "            return 'who'\n",
    "        elif question_word == \"when\":\n",
    "            return 'when'\n",
    "        elif question_word == \"where\":\n",
    "            return 'where'\n",
    "        elif question_word == \"what\":\n",
    "            return 'what'\n",
    "\n",
    "#Function to generate n-gram \n",
    "def gen_ngrams(text, n):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    tokens = [token for token in text.split(\" \") if token != \"\"]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "#Function to reformulate when question\n",
    "def when_query(input):\n",
    "    if input == r\"when (was|were)(.*)\":\n",
    "        input = r\"\\2 \\1\" \n",
    "    return input\n",
    "\n",
    "#Function to reformulate where question\n",
    "def where_query(input):\n",
    "    if input == r\"where (was|were)(.*)\":\n",
    "        input = r\"\\2 \\1\" \n",
    "    elif input == r\"where (was|were)(.*) (discovered|found|created|generated)\":\n",
    "        input = (r\"\\2 \\1 \\3\", 4)\n",
    "    return input\n",
    "\n",
    "#Function to reformulate what question\n",
    "def where_query(input):\n",
    "    if input == r\"what (is|was|were|does|did|can|could|should)(.*)\":\n",
    "        input = r\"\\2 \\1\" \n",
    "    return input\n",
    "\n",
    "#Function to reformulate who question\n",
    "def where_query(input):\n",
    "    if input == r\"who (is|was|were|can|could|should)(.*)\":\n",
    "        input = r\"\\2 \\1\" \n",
    "    return input\n",
    "\n",
    "#https://stackoverflow.com/questions/58151963/how-can-i-take-user-input-and-search-it-in-python\n",
    "\n",
    "#System can only accept questions that fall into these 4 categories:\n",
    "#accepted = ['Who', 'What', 'When', 'Where']\n",
    "\n",
    "#create a logging file\n",
    "logger = open('log-file.txt','w')\n",
    "logger.write('Starting New Log.....')\n",
    "\n",
    "\n",
    "# loops until exit\n",
    "while True:\n",
    "    #Takes user's input and searches wikipedia\n",
    "    ask = input('What would you like to learn today?\\n')\n",
    "    #tokenize the question and removve stopwords and use the remaining as keyword to search and filter\n",
    "    question_tokens = nltk.word_tokenize(ask)\n",
    "    keywords = [token for token in  question_tokens if token not in stopwords.words ('english')]\n",
    "    \n",
    "    # add to log file\n",
    "    logger.write('\\n' + ask)\n",
    "    \n",
    "    if (ask == 'exit'):\n",
    "        print('we will now exit')\n",
    "        break\n",
    "    \n",
    "    #check the type of question   \n",
    "    q_type = check_q_type(ask.lower())\n",
    "    #print(q_type)\n",
    "    \n",
    "    # logic for if a input is a who question\n",
    "    if q_type == 'who':\n",
    "        \n",
    "        # find any text and labels NER\n",
    "        text, label = find_ner(ask)\n",
    "        scraped_data = str(scrape_webpage(\"https://en.wikipedia.org/w/index.php?search={}\".format(text)))\n",
    "        sentences = sent_tokenize(scraped_data)\n",
    "        filtered_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if text in sentence:\n",
    "                for keyword in keywords:\n",
    "                    if keyword in sentence:\n",
    "                        filtered_sentences.append(sentence)\n",
    "        print(filtered_sentences)\n",
    "        ngram_string = \"\".join(filtered_sentences)\n",
    "        ngrams = gen_ngrams(ngram_string, 3)\n",
    "        print(ngrams)\n",
    "        print(text, label)\n",
    "        url = webbrowser.open(\"https://en.wikipedia.org/w/index.php?search={}\".format(text))\n",
    "            \n",
    "        \n",
    "    elif q_type == 'what':\n",
    "        text, label = find_ner(ask)\n",
    "        what_tag = [\"NORP\",\"PRODUCT\",\"EVENT\",\"WORK_OF_ART\",\"LAW\",\"LANGUAGE\",\"PERCENT\",\"MONEY\",\"QUANTITY\",\"ORDINAL\",\"CARDINAL\"]\n",
    "        scraped_data = str(scrape_webpage(\"https://en.wikipedia.org/w/index.php?search={}\".format(text)))\n",
    "        sentences = sent_tokenize(scraped_data)\n",
    "        #keywords = [\"was\", \"is\"]\n",
    "        filtered_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if text in sentence:\n",
    "                for keyword in keywords:\n",
    "                    if keyword in sentence:\n",
    "                        filtered_sentences.append(sentence)\n",
    "        print(filtered_sentences)\n",
    "        ngram_string = \"\".join(filtered_sentences)\n",
    "        ngrams = gen_ngrams(ngram_string, 3)\n",
    "        print(ngrams)\n",
    "        print(text, label)\n",
    "        url = webbrowser.open(\"https://en.wikipedia.org/w/index.php?search={}\".format(text))\n",
    "        \n",
    "    elif q_type == 'when':\n",
    "        text, label = find_ner(ask)\n",
    "        when_tag = [\"DATE\",\"TIME\"]\n",
    "        scraped_data = str(scrape_webpage(\"https://en.wikipedia.org/w/index.php?search={}\".format(text)))\n",
    "        sentences = sent_tokenize(scraped_data)\n",
    "        #keywords = [\"was\", \"is\", \"from\"]\n",
    "        #keywords = when_query(ask)\n",
    "        filtered_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if text in sentence:\n",
    "                for keyword in keywords:\n",
    "                    if keyword in sentence and find_ner2(sentence) in when_tag):\n",
    "                        filtered_sentences.append(sentence)\n",
    "        print(filtered_sentences)\n",
    "        ngram_string = \"\".join(filtered_sentences)\n",
    "        ngrams = gen_ngrams(ngram_string, 3)\n",
    "        print (keywords)\n",
    "        print(ngrams)\n",
    "        print(text, label)\n",
    "        url = webbrowser.open(\"https://en.wikipedia.org/w/index.php?search={}\".format(text))\n",
    "        \n",
    "    elif q_type == 'where':\n",
    "        text, label = find_ner(ask)\n",
    "        where_tag = [\"GPE\",\"ORG\",\"LOC\"]\n",
    "        scraped_data = str(scrape_webpage(\"https://en.wikipedia.org/w/index.php?search={}\".format(text)))\n",
    "        sentences = sent_tokenize(scraped_data)\n",
    "        #keywords = [\"was\", \"is\", \"near\"]\n",
    "        filtered_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if text in sentence:\n",
    "                for keyword in keywords:\n",
    "                    if keyword in sentence and (find_ner2(sentence) == \"GPE\" or find_ner2(sentence) == \"ORG\" or find_ner2(sentence) == \"LOC\"):\n",
    "                        filtered_sentences.append(sentence)\n",
    "        print(filtered_sentences)\n",
    "        ngram_string = \"\".join(filtered_sentences)\n",
    "        ngrams = gen_ngrams(ngram_string, 3)\n",
    "        print(ngrams)\n",
    "        print(text, label)\n",
    "        url = webbrowser.open(\"https://en.wikipedia.org/w/index.php?search={}\".format(text))\n",
    "        \n",
    "    else:\n",
    "        print('I can\\'t answer that question. Please try another question.')\n",
    "    \n",
    "\n",
    "# close the logging file after everything has been written        \n",
    "logger.close()\n",
    "        \n",
    "'''\n",
    "#Check if its an acceptable question\n",
    "if ask[0] in accepted:\n",
    "    #Then take question and use POS Tagger/NER to extract the main points its abouut\n",
    "    #Process it and then use that as the input for the wikipedia search\n",
    "#Use tagger on user input then use that as the variable to search wikipedia for\n",
    "elif ask[0] == 'exit':\n",
    "    print('Thank you! Goodbye.')\n",
    "else:\n",
    "    print(\"I am sorry, I don't know the answer\")    \n",
    "    \n",
    "#Scrapes the text from wikipedia site\n",
    "web_text = scrape_webpage(url)\n",
    "#Generates webpage's named entities.\n",
    "pprint([(X.text, X.label_) for X in url.ents])\n",
    "#Will extrace Part-of-speech of sentences from webpage\n",
    "#https://spacy.io/usage/spacy-101\n",
    "for sentences in web_text:\n",
    "    print(sentences.txt, sentences.pos_)\n",
    "'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
